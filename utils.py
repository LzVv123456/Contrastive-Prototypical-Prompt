import os
import math
import yaml
import json
import random
import argparse
import pprint
import subprocess
from operator import itemgetter
from datetime import datetime
from PIL import ImageFilter, ImageOps

import numpy as np
import torch
from torch import nn
from torch import Tensor
from torch.utils.tensorboard import SummaryWriter


def fix_random_seeds(seed=7):
    """
    Fix random seeds.
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    


def bool_flag(s):
    """
    Parse boolean arguments from the command line.
    """
    FALSY_STRINGS = {"off", "false", "0"}
    TRUTHY_STRINGS = {"on", "true", "1"}
    if s.lower() in FALSY_STRINGS:
        return False
    elif s.lower() in TRUTHY_STRINGS:
        return True
    else:
        raise argparse.ArgumentTypeError("invalid value for a boolean flag")


def mixup_data(x, y, alpha=1.0, use_cuda=True):
    '''Returns mixed inputs, pairs of targets, and lambda'''
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    if use_cuda:
        index = torch.randperm(batch_size).cuda()
    else:
        index = torch.randperm(batch_size)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam


class GaussianBlur(object):
    """
    Apply Gaussian Blur to the PIL image.
    """
    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):
        self.prob = p
        self.radius_min = radius_min
        self.radius_max = radius_max

    def __call__(self, img):
        do_it = random.random() <= self.prob
        if not do_it:
            return img

        return img.filter(
            ImageFilter.GaussianBlur(
                radius=random.uniform(self.radius_min, self.radius_max)
            )
        )


class Solarization(object):
    """
    Apply Solarization to the PIL image.
    """
    def __init__(self, p):
        self.p = p

    def __call__(self, img):
        if random.random() < self.p:
            return ImageOps.solarize(img)
        else:
            return img

class Cutout(object):
    def __init__(self, length):
        self.length = length

    def __call__(self, img):
        h, w = img.size(1), img.size(2)
        mask = np.ones((h, w), np.float32)
        y = np.random.randint(h)
        x = np.random.randint(w)

        y1 = np.clip(y - self.length // 2, 0, h)
        y2 = np.clip(y + self.length // 2, 0, h)
        x1 = np.clip(x - self.length // 2, 0, w)
        x2 = np.clip(x + self.length // 2, 0, w)

        mask[y1: y2, x1: x2] = 0.
        mask = torch.from_numpy(mask)
        mask = mask.expand_as(img)
        img *= mask
        return img


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):
    warmup_schedule = np.array([])
    warmup_iters = int(warmup_epochs * niter_per_ep)
    if warmup_epochs > 0:
        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)
    iters = np.arange(epochs * niter_per_ep - warmup_iters)
    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))
    schedule = np.concatenate((warmup_schedule, schedule))
    # assert len(schedule) == epochs * niter_per_ep
    return schedule


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def load_pretrained_weights(model, pretrained_weights, checkpoint_key, model_name, patch_size):
    if os.path.isfile(pretrained_weights):
        state_dict = torch.load(pretrained_weights, map_location="cpu")
        if checkpoint_key is not None and checkpoint_key in state_dict:
            print(f"Take key {checkpoint_key} in provided checkpoint dict")
            state_dict = state_dict[checkpoint_key]
        # remove `module.` prefix
        state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
        # remove `backbone.` prefix induced by multicrop wrapper
        state_dict = {k.replace("backbone.", ""): v for k, v in state_dict.items()}
        msg = model.load_state_dict(state_dict, strict=False)
        print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))
    else:
        print("Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.")
        url = None
        if model_name == "vit_small" and patch_size == 16:
            url = "dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth"
        elif model_name == "vit_small" and patch_size == 8:
            url = "dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth"
        elif model_name == "vit_base" and patch_size == 16:
            url = "dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth"
        elif model_name == "vit_base" and patch_size == 8:
            url = "dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth"
        elif model_name == "xcit_small_12_p16":
            url = "dino_xcit_small_12_p16_pretrain/dino_xcit_small_12_p16_pretrain.pth"
        elif model_name == "xcit_small_12_p8":
            url = "dino_xcit_small_12_p8_pretrain/dino_xcit_small_12_p8_pretrain.pth"
        elif model_name == "xcit_medium_24_p16":
            url = "dino_xcit_medium_24_p16_pretrain/dino_xcit_medium_24_p16_pretrain.pth"
        elif model_name == "xcit_medium_24_p8":
            url = "dino_xcit_medium_24_p8_pretrain/dino_xcit_medium_24_p8_pretrain.pth"
        elif model_name == "resnet50":
            url = "dino_resnet50_pretrain/dino_resnet50_pretrain.pth"
        if url is not None:
            print("Since no pretrained weights have been provided, we load the reference pretrained DINO weights.")
            state_dict = torch.hub.load_state_dict_from_url(url="https://dl.fbaipublicfiles.com/dino/" + url)
            model.load_state_dict(state_dict, strict=True)
        else:
            print("There is no reference weights available for this model => We use random weights.")


def get_sha():
    cwd = os.path.dirname(os.path.abspath(__file__))

    def _run(command):
        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()
    sha = 'N/A'
    diff = "clean"
    branch = 'N/A'
    try:
        sha = _run(['git', 'rev-parse', 'HEAD'])
        subprocess.check_output(['git', 'diff'], cwd=cwd)
        diff = _run(['git', 'diff-index', 'HEAD'])
        diff = "has uncommited changes" if diff else "clean"
        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])
    except Exception:
        pass
    message = f"sha: {sha}, status: {diff}, branch: {branch}"
    return message

    
def cosine_similarity(input, target):
    """
    input: (dim_input, embed_dim)
    target: (dim_ouput, embed_dim)
    similarity: (dim_input, dim_ouput)
    """
    input_norm = nn.functional.normalize(input, dim=1, p=2)
    target_norm = nn.functional.normalize(target, dim=1, p=2)
    similarity = _safe_matmul(input_norm, target_norm)
    similarity  = torch.nan_to_num(similarity)
    eps = 1e-8
    similarity[similarity<=eps] = eps
    # similarity[torch.isnan(similarity)] = eps
    return similarity


def _safe_matmul(x: Tensor, y: Tensor) -> Tensor:
    """Safe calculation of matrix multiplication.
    If input is float16, will cast to float32 for computation and back again.
    """
    if x.dtype == torch.float16 or y.dtype == torch.float16:
        return torch.matmul(x.float(), y.float().t()).half()
    return torch.matmul(x, y.t())


def check_proto_similarity(prototypes):
    # set diagnol to zero
    norm_prototypes = nn.functional.normalize(prototypes, dim=1, p=2)
    similarity_matrix = torch.mm(norm_prototypes, norm_prototypes.t())
    similarity_matrix = torch.diagonal_scatter(similarity_matrix, torch.zeros(similarity_matrix.size(0)), 0)
    print('matrix size:{}'.format(similarity_matrix.size()))
    print('min similarity:{}'.format(torch.min(similarity_matrix[similarity_matrix>0])))
    print('max similarity:{}'.format( torch.max(similarity_matrix)))
    print('average similarity: {}'.format(torch.mean(similarity_matrix[similarity_matrix>0])))


def adjust_order(contents, orders):
    """
    Resort the contents from the given order to ascending order
    """
    assert len(contents) == len(orders)
    contents = [(contents[i], orders[i]) for i in range(len(orders))]
    # sort prompts with ascending order regarding the class_id. e.g. 0,1,2 ... total_nc-1
    contents = sorted(contents, key=itemgetter(1))
    # remove class_id
    contents = [item[0] for item in contents]
    return contents


def measure_distance(similarity, batch_features, prototypes):
    """
    batch_features: (batch_size, feature_size)
    prototypes: (prototpyes up to now, feature_size)
    """

    if similarity in ['l1', 'l2']:
        p = float(similarity[-1])
        distances = torch.cdist(batch_features.cuda(), prototypes.cuda(), p=p)  # dists: (batch_size, protos num up to now)

    elif similarity == 'cosine':
        batch_features = nn.functional.normalize(batch_features, dim=1, p=2)
        prototypes = nn.functional.normalize(prototypes, dim=1, p=2)
        sim = torch.mm(batch_features, prototypes.t())
        distances = torch.ones(sim.size()).cuda() - sim  # convert similarity to distances.

    else:
        raise NotADirectoryError

    # sanity check
    assert distances.size() == (batch_features.size(0), prototypes.size(0))
    return distances


def accuracy(output, target, topk=(1,)):
    """Computes the accuracy over the k top predictions for the specified values of k"""
    maxk = max(topk)
    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.reshape(1, -1).expand_as(pred))
    return [correct[:k].reshape(-1).float().sum(0) for k in topk]


def cal_acc(args, distances, gt_labels, class_seq):
    assert len(class_seq) == distances.shape[-1]
    # get topk minumum distances
    topk = min(args.topk, distances.size(-1))
    topk_values, topk_indexes = torch.topk(distances, largest=False, k=topk, dim=-1)
    top1_values, top1_indexes = torch.topk(distances, largest=False, k=1, dim=-1)

    # get top1 label
    class_seq = torch.tensor(class_seq)
    top1_labels = [class_seq[i] for i in top1_indexes.squeeze()]
    top1_labels = torch.stack(top1_labels, dim=0).cuda()
    topk_labels = [class_seq[topk_indexes[i, :]] for i in range(len(topk_indexes))]
    topk_labels = torch.stack(topk_labels, dim=0).cuda()

    top1_correct = (top1_labels.squeeze() == gt_labels).sum()
    topk_correct = [1 if gt_labels[idx] in topk_labels[idx,:] else 0 for idx in range(len(gt_labels))]
    topk_correct = np.sum(topk_correct)
    return (top1_correct, topk_correct), (topk_indexes, topk_labels, top1_labels.squeeze())


def cal_acc_mc(args, distances, gt_labels, mc_seq, mc_proto_mapping):
    assert len(mc_seq) == distances.shape[-1]
    mc_seq = torch.tensor(mc_seq)
    
    topk = min(args.topk, distances.size(-1))
    # get topk minumum distances
    topk_values, topk_indexes = torch.topk(distances, largest=False, k=topk, dim=-1)
    top1_values, top1_indexes = torch.topk(distances, largest=False, k=1, dim=-1)
    # get top1 label
    top1_labels = [mc_proto_mapping[mc_seq[i]] for i in top1_indexes.squeeze()]
    top1_labels = torch.stack(top1_labels, dim=0).cuda()
    # get topk label
    topk_labels = [mc_proto_mapping[mc_seq[i.to('cpu')]] for i in topk_indexes]
    topk_labels = torch.stack(topk_labels, dim=0).cuda()

    # count top1 correct 
    top1_correct = (top1_labels.squeeze() == gt_labels).sum()
    # count topk correct 
    topk_correct = [1 if gt_labels[idx] in topk_labels[idx] else 0 for idx in range(len(gt_labels))]
    topk_correct = np.sum(topk_correct)

    return (top1_correct, topk_correct), (topk_indexes, topk_labels, top1_labels.squeeze())


class save_helper(object):
    def __init__(self, args):
        self.args = args
        if not self.args.infer_path:
            self.init_save_structure()
            self.tb = SummaryWriter(log_dir=self.args.output_dir)  # tensorboard
            config_text = "\n".join("%s: %s" % (k, str(v)) for k, v in sorted(dict(vars(self.args)).items()))
            print(config_text)
            self.save_config(config_text)
        else: 
            # infer mode, infer a given path
            self.args.output_dir = self.args.infer_path
            self.tb = None

    def init_save_structure(self):
        # level 1: dataset
        self.args.output_dir = self.args.output_dir + '/' + str(self.args.dataset)
        # level 2: architecture
        self.args.output_dir = self.args.output_dir + '/' + str(self.args.arch) + '-' + str(self.args.patch_size)
        # level 3: 
        if self.args.exp_name:
            self.args.output_dir = self.args.output_dir + '/' + str(self.args.exp_name)
        else:
            # datetime object containing current date and time
            now = datetime.now()
            # dd/mm/YY H:M:S
            dt_string = now.strftime("%d-%m-%Y-%H:%M:%S")
            self.args.output_dir = self.args.output_dir + '/' + str(dt_string)
        
        if not os.path.exists(self.args.output_dir):
            os.makedirs(self.args.output_dir)


    def save_config(self, context):
        with open(self.args.output_dir + "/config.txt", "w") as f:
            f.write(context)

    def save_prototype(self, context, name=None):
        if name:
            save_name = str(name)+'-'+'prototypes.pt'
        else:
            save_name = 'prototypes.pt'
        torch.save(context, self.args.output_dir+'/'+save_name)
    
    def save_prompt(self, context, name=None):
        if name:
            save_name = str(name)+'-'+'prompts.pt'
        else:
            save_name = 'prompts.pt'
        torch.save(context, self.args.output_dir+'/'+save_name)

    def save_torch(self, context, name=None):
        torch.save(context, self.args.output_dir+'/'+name+'.pt')

    def save_txt(self, context, name=None):
        with open(self.args.output_dir+'/'+name +'.txt', 'w') as fp:
            pprint.pprint(context, stream=fp, sort_dicts=False)

    def save_yaml(self, context, name=None):
        with open(self.args.output_dir+'/'+name+'.yaml', 'w', encoding = "utf-8") as fp:
            yaml.dump(context, fp, default_flow_style=False, allow_unicode = True, encoding = None)

    def save_json(self, context, name=None):
        with open(self.args.output_dir+'/'+name+'.json', 'w') as fp:
            json.dump(context, fp)


class continual_metric_helper(object):
    def __init__(self, args):
        self.args = args
        self.total_sessions = args.task_num + (1 if args.fg_nc > 0 else 0)
        self.current_session = None
        self.current_task = None
        self.acc_report = None
        self.forget_report = None
        self.metric_all_sessions = {}
        self.raw_result_all_sessions = {}
    
    def set_current_session(self, session_id):
        self.current_session = session_id
        self.metric_all_sessions[self.current_session] = {}
        self.raw_result_all_sessions[self.current_session] = {}

    def set_current_task(self, task_id):
        self.current_task = task_id
        assert self.current_task <= self.current_session, print('Wrong task or session!')
        self.metric_all_sessions[self.current_session][self.current_task] = {'top1':0, 'topk':0, 'total':0}
        self.raw_result_all_sessions[self.current_session][self.current_task] = {'predict':None, 'gt':None}

    def update_raw(self, predict, gt):
        assert len(predict) == len(gt)
        if self.raw_result_all_sessions[self.current_session][self.current_task]['predict'] is None:
            self.raw_result_all_sessions[self.current_session][self.current_task]['predict'] = predict.unsqueeze(0)
            self.raw_result_all_sessions[self.current_session][self.current_task]['gt'] = gt.unsqueeze(0)
        else:
            # print(self.raw_result_all_sessions[self.current_session][self.current_task]['predict'].size(), \
            # predict.unsqueeze(0).size())

            self.raw_result_all_sessions[self.current_session][self.current_task]['predict'] = \
            torch.cat([self.raw_result_all_sessions[self.current_session][self.current_task]['predict'], \
            predict.unsqueeze(0)], dim=-1)
            self.raw_result_all_sessions[self.current_session][self.current_task]['gt'] = \
            torch.cat([self.raw_result_all_sessions[self.current_session][self.current_task]['gt'], \
            gt.unsqueeze(0)], dim=-1)

    def update(self, top1_correct, topk_correct, total):
        self.metric_all_sessions[self.current_session][self.current_task]['top1'] += top1_correct
        self.metric_all_sessions[self.current_session][self.current_task]['topk'] += topk_correct
        self.metric_all_sessions[self.current_session][self.current_task]['total'] += total

    def measure_acc(self, key='top1'):
        self.session_wise_avg_acc = {}
        self.session_wise_acc_collect = {}
        for session_id, session_dict in self.metric_all_sessions.items():
            all_task_acc = []  # this is a macro way of calculating acc
            for task_id, task_dict in session_dict.items():
                assert int(task_id) <= int(session_id)
                task_acc = task_dict[key]/task_dict['total']
                try:
                    task_acc = task_acc.item()
                except:
                    pass
                all_task_acc.append(np.around(task_acc, 4))
            self.session_wise_acc_collect[session_id] = all_task_acc
            self.session_wise_avg_acc[session_id] = np.around(np.mean(all_task_acc), 4) 
        
        self.total_avg_acc = np.mean([item for _, item in self.session_wise_avg_acc.items()])
        self.acc_report = {'Session-wise acc collection': self.session_wise_acc_collect, \
                           'Session-wise average acc': self.session_wise_avg_acc, \
                           'Total average acc': np.around(self.total_avg_acc, 4)}
        return self.acc_report
        
                
    def measure_forget(self):  # use measure forget right after measure acc
        self.session_wise_avg_forget = {}
        self.session_wise_forget_collect = {}
        if len(self.session_wise_acc_collect.keys()) > 1:
            for session_id in range(1, len(self.session_wise_acc_collect.keys())):
                acc_collection = self.session_wise_acc_collect[session_id]
                all_task_forget = []
                for task_id, task_acc in enumerate(acc_collection[:-1]):
                    all_acc_for_task = []
                    for idx in range(session_id):
                        try:
                            all_acc_for_task.append(self.session_wise_acc_collect[idx][task_id])
                        except:
                            pass
                    task_forget = max(np.max(all_acc_for_task) - task_acc, 0)
                    all_task_forget.append(np.around(task_forget, 4))
                
                self.session_wise_forget_collect[session_id] = all_task_forget
                self.session_wise_avg_forget[session_id] = np.around(np.mean(all_task_forget), 4)
            
            self.total_avg_forget = np.mean([item for _, item in self.session_wise_avg_forget.items()])
            self.forget_report = {'Session-wise forget collection': self.session_wise_forget_collect, \
                                'Session-wise average forget': self.session_wise_avg_forget, \
                                'Total average forget': np.around(self.total_avg_forget, 4)}
        else:
            pass
        
        return self.forget_report
         

    def get_result_report(self):
        # measure top1
        try:
            top1_acc_report = self.measure_acc(key='top1')
        except:
            top1_acc_report = None
        try:
            top1_forget_report = self.measure_forget()
        except:
            top1_forget_report = None

        # measure topk
        try:
            topk_acc_report = self.measure_acc(key='topk')
        except:
            topk_acc_report = None
        try:
            topk_forget_report = self.measure_forget()
        except:
            topk_forget_report = None

        self.result_report = {'Accuracy': {'Top-1': top1_acc_report, 'Top-'+str(self.args.topk): topk_acc_report},
                              'Forget': {'Top-1': top1_forget_report, 'Top-'+str(self.args.topk): topk_forget_report}}
